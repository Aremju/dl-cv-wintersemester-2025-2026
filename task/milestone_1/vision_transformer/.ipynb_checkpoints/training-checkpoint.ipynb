{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65ad528b-3221-42ee-9400-b6586f9e22a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 10:58:41.103389: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-16 10:58:41.146868: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-16 10:58:42.157084: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor, Trainer, TrainingArguments\n",
    "from torchvision.transforms import v2\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddf2dc5-3ae8-456c-b9ef-fcd504359c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b33a5466c2fe4feb9b7b791e03d18d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42cbf6fd93b54db1a99b773faa783c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading dataset\n",
    "animal_dataset = load_dataset(\"imagefolder\", data_dir=\"../../../data/animal_images\")\n",
    "animal_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66161a9c-e2b0-4355-b320-54bce6fdeadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking example structure of dataset in training data\n",
    "features = animal_dataset[\"train\"].features\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe0fff1-24e9-4a57-aeac-d71d48498c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of example content of dataset\n",
    "animal_dataset['train'][10]['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dc431e-0845-4b8c-98d0-77ec7fd5d599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into seperate datasets to parse onto trainer later on\n",
    "train_data = animal_dataset[\"train\"]\n",
    "validation_data = animal_dataset[\"validation\"]\n",
    "test_data = animal_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7116a53-c51a-4ecf-b64d-aaf190172eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label mapping for model (label-name -> index)\n",
    "id2label = {id: label for id, label in enumerate(train_data.features[\"label\"].names)}\n",
    "label2id = {label: id for id, label in id2label.items()}\n",
    "id2label, id2label[train_data[0][\"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba831ab1-bca7-4a97-872d-d309ee268ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model information\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "# Defining Cross-Entropy-Loss as loss function\n",
    "model.loss_fn = nn.CrossEntropyLoss()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218f636b-d58f-49b0-acfd-6bbba9bfd9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading processor\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224', do_rescale = False, return_tensors = 'pt')\n",
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eebd5e1-a901-4d00-91cb-2250d0b6f09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms \n",
    "\n",
    "# Get configurations from ViT processor\n",
    "size = processor.size.get(\"height\", 224)\n",
    "image_mean, image_std = processor.image_mean, processor.image_std\n",
    "\n",
    "# Normalization and augmentation transformations\n",
    "transformations = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.RandomResizedCrop(size),\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=image_mean, std=image_std),\n",
    "    ]),\n",
    "    \"validation\": transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.CenterCrop(size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=image_mean, std=image_std),\n",
    "    ]),\n",
    "    \"test\": transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.CenterCrop(size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=image_mean, std=image_std),\n",
    "    ]),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90ddc24-ad8a-4a99-8e5c-2750d8192df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the specified transformation configuration and apply it to the given example for \n",
    "def transform(examples, kind=\"train\"):\n",
    "    transform_fn = transformations.get(kind, transformations[\"train\"])\n",
    "    examples[\"pixel_values\"] = [transform_fn(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
    "    return examples\n",
    "    \n",
    "# Attaching right transformations to each dataset\n",
    "train_data.set_transform(lambda examples: transform(examples, \"train\"))\n",
    "validation_data.set_transform(lambda examples: transform(examples, \"validation\"))\n",
    "test_data.set_transform(lambda examples: transform(examples, \"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59635f12-a689-46e9-8f85-27d59fe24878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Function fixes issue with data-types, as default trainer collate function is not aware how to stack the tensors from our dataset\n",
    "def collate_fn(examples):\n",
    "    # Stacks the pixel values of all examples into a single tensor and collects labels into a tensor\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee559da-0255-4b70-92e6-24dfa05ec5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "labels  = animal_dataset['train'].features['label'].names\n",
    "\n",
    "# Loading model with proper label mapping\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224\", \n",
    "    num_labels = len(labels),\n",
    "    id2label=id2label, \n",
    "    label2id=label2id, \n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bc16aa-fd32-4f61-8154-a2c9dc912fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# Training params\n",
    "TRAINING_STRATEGY = \"linear\"\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "STEPS = 200\n",
    "\n",
    "\n",
    "\n",
    "train_configs = {\n",
    "    \"prod\": TrainingArguments(\n",
    "        output_dir=\"output-models\",\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        eval_strategy=\"steps\",\n",
    "        save_steps=STEPS,\n",
    "        eval_steps=STEPS,\n",
    "        # warmup_steps=STEPS, \n",
    "        num_train_epochs=EPOCHS,\n",
    "        fp16=True,\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=STEPS,\n",
    "        learning_rate=2e-4,\n",
    "        lr_scheduler_type=\"cosine\", # \"linear\" # \"constant\" \n",
    "        # optim=\"adamw_torch\",\n",
    "        remove_unused_columns=False,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        load_best_model_at_end=True,\n",
    "        greater_is_better=True,\n",
    "        report_to=\"tensorboard\",\n",
    "        save_strategy=\"steps\",\n",
    "        save_total_limit=3,\n",
    "        seed = 123\n",
    "    ),\n",
    "\n",
    "    \"linear\": TrainingArguments(\n",
    "        output_dir=\"output-models\",\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        eval_strategy=\"steps\",\n",
    "        save_steps=STEPS,\n",
    "        eval_steps=STEPS,\n",
    "        # warmup_steps=STEPS, \n",
    "        num_train_epochs=EPOCHS,\n",
    "        fp16=True,\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=STEPS,\n",
    "        learning_rate=2e-4,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        # optim=\"adamw_torch\",\n",
    "        remove_unused_columns=False,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        load_best_model_at_end=True,\n",
    "        greater_is_better=True,\n",
    "        report_to=\"tensorboard\",\n",
    "        save_strategy=\"steps\",\n",
    "        save_total_limit=3,\n",
    "        seed = 123\n",
    "    ),\n",
    "\n",
    "    \"prod_epoch\": TrainingArguments(\n",
    "        output_dir=\"output-models\",\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        eval_strategy=\"epoch\",\n",
    "        eval_steps=1,\n",
    "        # warmup_steps=STEPS, \n",
    "        num_train_epochs=EPOCHS,\n",
    "        fp16=True,\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=100,\n",
    "        learning_rate=2e-4,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        remove_unused_columns=False,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        load_best_model_at_end=True,\n",
    "        greater_is_better=True,\n",
    "        report_to=\"tensorboard\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=3,\n",
    "        seed = 123\n",
    "    ),\n",
    "\n",
    "    \"debug\": TrainingArguments(\n",
    "        output_dir=\"output-models\",\n",
    "        per_device_train_batch_size=32,\n",
    "        eval_strategy=\"steps\",\n",
    "        num_train_epochs=0.01,\n",
    "        fp16=True,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=10,\n",
    "        eval_steps=10,\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=10,\n",
    "        learning_rate=2e-4,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        remove_unused_columns=False,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        load_best_model_at_end=True,\n",
    "        greater_is_better=True,\n",
    "        report_to=\"tensorboard\",\n",
    "        save_total_limit=3,\n",
    "        seed = 123\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3671da-5f0c-4997-88cf-19af79dbf4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load standard evaluation metrics\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "\n",
    "# Function called after completing eval strategy rule\n",
    "def compute_metrics(eval_predictions):\n",
    "    # Accessing model predictions\n",
    "    model_calculations, true_labels = eval_predictions\n",
    "    # Takes the model output with the highest value (so the most likely class to be predicted)\n",
    "    model_predictions = np.argmax(model_calculations, axis=-1)\n",
    "\n",
    "    \"\"\"# Creating confusion matrix to check correctness of outputs (lines = true_labels, rows = model_predictions)\n",
    "    confusion = confusion_matrix(true_labels, model_predictions)\n",
    "\n",
    "    # Debug confusion info\n",
    "    print(confusion.shape)\n",
    "    print(confusion)\n",
    "\n",
    "    # Manual calculations of result categories\n",
    "    TP = np.diag(confusion).sum()\n",
    "    FP = confusion.sum(axis=0) - np.diag(confusion)\n",
    "    FN = confusion.sum(axis=1) - np.diag(confusion)\n",
    "    TN = confusion.sum() - (FP + FN + np.diag(confusion)).sum()\n",
    "\n",
    "    # Test verify accurracy\n",
    "    acc = (TP + TN) / np.sum(confusion)\n",
    "    print(f\"Accurracy: {acc * 100:.2f}%\") \"\"\"\n",
    "    \n",
    "    # Computing all predefined metrics\n",
    "    return {\n",
    "        \"accuracy\": accuracy.compute(predictions=model_predictions, references=true_labels)[\"accuracy\"],\n",
    "        \"precision\": precision.compute(predictions=model_predictions, references=true_labels, average=\"weighted\")[\"precision\"],\n",
    "        \"recall\": recall.compute(predictions=model_predictions, references=true_labels, average=\"weighted\")[\"recall\"],\n",
    "        \"f1\": f1.compute(predictions=model_predictions, references=true_labels, average=\"weighted\")[\"f1\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b632017f-c77c-4f97-acfa-73e5bf0f81e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing and starting training\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    train_configs.get(TRAINING_STRATEGY),\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=validation_data,\n",
    "    data_collator=collate_fn,\n",
    "    tokenizer=processor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.005)],\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d922da5-af95-4398-a9ed-3f6955066fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    classification_report,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Evaluation of model\n",
    "predictions = trainer.predict(test_data)\n",
    "\n",
    "# Ids of True Labels\n",
    "labels_true = predictions.label_ids\n",
    "# Ids of predicted labels\n",
    "labels_pred = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "# Configs for label <-> id mapping\n",
    "id2label = model.config.id2label\n",
    "label2id = model.config.label2id\n",
    "\n",
    "# Plot of confusion matrix\n",
    "result_confusion = confusion_matrix(labels_true, labels_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=result_confusion, display_labels=labels)\n",
    "disp.plot(cmap=\"Blues\", xticks_rotation=70, values_format=\"d\", ax=ax)\n",
    "\n",
    "ax.set_xlabel(\"Predicted Labels\", fontsize=15, labelpad=15)\n",
    "ax.set_ylabel(\"True Labels\", fontsize=15, labelpad=15)\n",
    "ax.set_title(\"Confusion Matrix (Animal_Images)\", fontsize=16, pad=10)\n",
    "\n",
    "ax.tick_params(axis=\"x\", labelsize=11, pad=5)\n",
    "ax.tick_params(axis=\"y\", labelsize=11, pad=5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Output of sklearn classification report\n",
    "report = classification_report(\n",
    "    labels_true,\n",
    "    labels_pred,\n",
    "    target_names=labels,\n",
    "    output_dict=True,\n",
    ")\n",
    "\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "print(f\"\\n\\n scikit-learn report: \\n{report_df}\")\n",
    "\n",
    "report_df.to_csv(\"output-models/classification_report.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc0ef6c-34fe-41b2-b09b-92afda424873",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
