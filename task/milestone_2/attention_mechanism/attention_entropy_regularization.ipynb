{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ad528b-3221-42ee-9400-b6586f9e22a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import TFViTForImageClassification, ViTConfig, ViTImageProcessor, Trainer, TrainingArguments\n",
    "from torchvision.transforms import v2\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddf2dc5-3ae8-456c-b9ef-fcd504359c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset\n",
    "animal_dataset = load_dataset(\"imagefolder\", data_dir=\"../../../data/animal_images\")\n",
    "animal_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66161a9c-e2b0-4355-b320-54bce6fdeadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking example structure of dataset in training data\n",
    "features = animal_dataset[\"train\"].features\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe0fff1-24e9-4a57-aeac-d71d48498c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of example content of dataset\n",
    "animal_dataset['train'][10]['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dc431e-0845-4b8c-98d0-77ec7fd5d599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into seperate datasets to parse onto trainer later on\n",
    "train_data = animal_dataset[\"train\"]\n",
    "validation_data = animal_dataset[\"validation\"]\n",
    "test_data = animal_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7116a53-c51a-4ecf-b64d-aaf190172eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label mapping for model (label-name -> index)\n",
    "id2label = {id: label for id, label in enumerate(train_data.features[\"label\"].names)}\n",
    "label2id = {label: id for id, label in id2label.items()}\n",
    "id2label, id2label[train_data[0][\"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218f636b-d58f-49b0-acfd-6bbba9bfd9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading processor\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224', do_rescale = False, return_tensors = 'pt')\n",
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eebd5e1-a901-4d00-91cb-2250d0b6f09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms \n",
    "\n",
    "# Get configurations from ViT processor\n",
    "size = processor.size.get(\"height\", 224)\n",
    "image_mean, image_std = processor.image_mean, processor.image_std\n",
    "\n",
    "# Normalization and augmentation transformations\n",
    "transformations = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.RandomResizedCrop(size),\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3),\n",
    "        transforms.Resize(size),\n",
    "        transforms.CenterCrop(size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=image_mean, std=image_std),\n",
    "    ]),\n",
    "    \"validation\": transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.CenterCrop(size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=image_mean, std=image_std),\n",
    "    ]),\n",
    "    \"test\": transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.CenterCrop(size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=image_mean, std=image_std),\n",
    "    ]),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90ddc24-ad8a-4a99-8e5c-2750d8192df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the specified transformation configuration and apply it to the given example for \n",
    "def transform(examples, kind=\"train\"):\n",
    "    transform_fn = transformations.get(kind, transformations[\"train\"])\n",
    "    examples[\"pixel_values\"] = [transform_fn(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
    "    return examples\n",
    "    \n",
    "# Attaching right transformations to each dataset\n",
    "train_data.set_transform(lambda examples: transform(examples, \"train\"))\n",
    "validation_data.set_transform(lambda examples: transform(examples, \"validation\"))\n",
    "test_data.set_transform(lambda examples: transform(examples, \"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59635f12-a689-46e9-8f85-27d59fe24878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Function fixes issue with data-types, as default trainer collate function is not aware how to stack the tensors from our dataset\n",
    "def collate_fn(examples):\n",
    "    # Stacks the pixel values of all examples into a single tensor and collects labels into a tensor\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels, \"interpolate_pos_encoding\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee559da-0255-4b70-92e6-24dfa05ec5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model loading\n",
    "import tensorflow as tf\n",
    "from transformers import ViTForImageClassification, ViTConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import types\n",
    "\n",
    "labels = animal_dataset['train'].features['label'].names\n",
    "BASE_MODEL_CONF = \"google/vit-base-patch16-224\"\n",
    "\n",
    "PATCH_SIZE = 16\n",
    "LAMBDA_H = -1.0\n",
    "\n",
    "# Label mapping and patch size are defined in config\n",
    "config = ViTConfig.from_pretrained(BASE_MODEL_CONF)\n",
    "config.patch_size = PATCH_SIZE\n",
    "config.num_labels = len(labels)\n",
    "config.id2label = id2label\n",
    "config.label2id = label2id\n",
    "\n",
    "# Eager attention implementation to access attention outputs in model\n",
    "config._attn_implementation = \"eager\"\n",
    "\n",
    "# Loading model with proper label mapping + configurable patch size\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    BASE_MODEL_CONF,\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b31dba7-a6d3-48f3-a56f-dabe8ce23644",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# Training params\n",
    "TRAINING_STRATEGY = \"fixed\"\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "STEPS = 200\n",
    "\n",
    "output_dir = f\"output_vit_attention_mechanism_entropy_{LAMBDA_H}_scaling\"\n",
    "\n",
    "train_configs = {\n",
    "    \"fixed\": TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_steps=1,\n",
    "        eval_steps=1,\n",
    "        # warmup_steps=STEPS, \n",
    "        num_train_epochs=EPOCHS,\n",
    "        fp16=True,\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=STEPS,\n",
    "        learning_rate=2e-4,\n",
    "        # optim=\"adamw_torch\",\n",
    "        remove_unused_columns=False,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        load_best_model_at_end=True,\n",
    "        greater_is_better=True,\n",
    "        report_to=\"tensorboard\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=3,\n",
    "        seed = 123\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3671da-5f0c-4997-88cf-19af79dbf4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load standard evaluation metrics\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "\n",
    "# Function called after completing eval strategy rule\n",
    "def compute_metrics(eval_predictions):\n",
    "    # Accessing model predictions\n",
    "    model_calculations, true_labels = eval_predictions\n",
    "    # Takes the model output with the highest value (so the most likely class to be predicted)\n",
    "    model_predictions = np.argmax(model_calculations, axis=-1)\n",
    "\n",
    "    # Computing all predefined metrics\n",
    "    return {\n",
    "        \"accuracy\": accuracy.compute(predictions=model_predictions, references=true_labels)[\"accuracy\"],\n",
    "        \"precision\": precision.compute(predictions=model_predictions, references=true_labels, average=\"weighted\")[\"precision\"],\n",
    "        \"recall\": recall.compute(predictions=model_predictions, references=true_labels, average=\"weighted\")[\"recall\"],\n",
    "        \"f1\": f1.compute(predictions=model_predictions, references=true_labels, average=\"weighted\")[\"f1\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0936d309-ce74-4a46-8aed-f183b4322134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def entropy_regularized_loss(lambda_H: float = 0.1, eps: float = 1e-8):\n",
    "    \"\"\"\n",
    "    Creates a compute_loss_func that adds attention entropy regularization\n",
    "    to the model's base loss.\n",
    "\n",
    "    Args:\n",
    "        lambda_H: weight of the entropy regularization term\n",
    "        eps: small constant to avoid log(0) when computing entropy\n",
    "    \"\"\"\n",
    "    def compute_loss_func(outputs, labels, num_items_in_batch=None):\n",
    "        # Encapsulation of basic compute_loss_func to use extra parameters lambda_H and eps\n",
    "        # Use model provided loss if available else calculates by itself via logits and labels\n",
    "        base_loss = getattr(outputs, \"loss\", None)\n",
    "        if base_loss is None:\n",
    "            logits = outputs.logits\n",
    "            base_loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        # Attentions must be returned by the forward pass\n",
    "        attentions = getattr(outputs, \"attentions\", None)\n",
    "        if attentions is None:\n",
    "            raise RuntimeError(\n",
    "                \"Attentions are None. Set model.config.output_attentions = True.\"\n",
    "            )\n",
    "\n",
    "        # Compute mean attention entropy across layers\n",
    "        entropy_sum = 0.0\n",
    "        for A in attentions:\n",
    "            # Clamp to avoid log(0)\n",
    "            A = A.clamp(min=eps)\n",
    "\n",
    "            # Entropy is H = -sum_j A_ij * log(A_ij)\n",
    "            H = -(A * A.log()).sum(dim=-1)\n",
    "            entropy_sum += H.mean()\n",
    "\n",
    "        entropy_mean = entropy_sum / len(attentions)\n",
    "\n",
    "        # Regularization of loss\n",
    "        # + lambda_H * entropy which penalizes high entropy resulting in sharper attention\n",
    "        # - lambda_H * entropy which encourages high entropy resulting in more uniform attention\n",
    "        total_loss = base_loss + lambda_H * entropy_mean\n",
    "\n",
    "        # print(f\"lambda: {lambda_H}, total_loss: {total_loss}\")\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "    return compute_loss_func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b632017f-c77c-4f97-acfa-73e5bf0f81e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the model always return attentions\n",
    "model.config.output_attentions = True\n",
    "model.config.return_dict = True\n",
    "\n",
    "# Initializes trainer with custom entropy loss regularization\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_configs.get(TRAINING_STRATEGY),\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=validation_data,\n",
    "    data_collator=collate_fn,\n",
    "    tokenizer=processor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.001)],\n",
    "    compute_loss_func=entropy_regularized_loss(lambda_H=LAMBDA_H),\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d922da5-af95-4398-a9ed-3f6955066fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    classification_report,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Evaluation of model\n",
    "predictions = trainer.predict(test_data)\n",
    "\n",
    "# Ids of True Labels\n",
    "labels_true = predictions.label_ids\n",
    "# Ids of predicted labels\n",
    "labels_pred = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "# Configs for label <-> id mapping\n",
    "id2label = model.config.id2label\n",
    "label2id = model.config.label2id\n",
    "\n",
    "# Plot of confusion matrix\n",
    "result_confusion = confusion_matrix(labels_true, labels_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=result_confusion, display_labels=labels)\n",
    "disp.plot(cmap=\"Blues\", xticks_rotation=70, values_format=\"d\", ax=ax)\n",
    "\n",
    "ax.set_xlabel(\"Predicted Labels\", fontsize=15, labelpad=15)\n",
    "ax.set_ylabel(\"True Labels\", fontsize=15, labelpad=15)\n",
    "ax.set_title(\"Confusion Matrix (Animal_Images)\", fontsize=16, pad=10)\n",
    "\n",
    "ax.tick_params(axis=\"x\", labelsize=11, pad=5)\n",
    "ax.tick_params(axis=\"y\", labelsize=11, pad=5)\n",
    "plt.tight_layout()\n",
    "\n",
    "output_path = f\"{output_dir}/confusion_matrix.png\"\n",
    "plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Output of sklearn classification report\n",
    "report = classification_report(\n",
    "    labels_true,\n",
    "    labels_pred,\n",
    "    target_names=labels,\n",
    "    output_dict=True,\n",
    ")\n",
    "\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "print(f\"\\n\\n scikit-learn report: \\n{report_df}\")\n",
    "\n",
    "report_df.to_csv(f\"{output_dir}/classification_report.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc0ef6c-34fe-41b2-b09b-92afda424873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2214d33b-a3ae-479d-856e-f34e251faaeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919ccce1-ed22-4228-84b8-fac39d938090",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
