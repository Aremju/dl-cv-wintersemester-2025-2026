{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fa5c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import (\n",
    "    ViTConfig,\n",
    "    ViTForImageClassification,\n",
    "    ViTImageProcessor,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e28f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Pfade & Hyperparameter\n",
    "# =============================\n",
    "\n",
    "# Pfade anpassen\n",
    "IMAGENET_ROOT = \"../../../data/imagenet\"          # imagefolder-Style: train/, val/\n",
    "ANIMAL_ROOT = \"../../../data/animal_images\" # wie in deinem bisherigen Notebook\n",
    "\n",
    "OUTPUT_IMAGENET_MODEL_DIR = \"./vit_patchX_imagenet\"\n",
    "OUTPUT_ANIMAL_MODEL_DIR = \"./vit_patchX_animals\"\n",
    "\n",
    "# Modell-/Trainings-Config\n",
    "PATCH_SIZE = 8       # <--- HIER deine gewünschte Patch-Size eintragen\n",
    "IMAGE_SIZE = 224      # Input-Resolution\n",
    "IMAGENET_NUM_EPOCHS = 10\n",
    "ANIMAL_NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 5e-5\n",
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7e244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# ImageNet laden (imagefolder)\n",
    "# =============================\n",
    "\n",
    "# Erwartete Struktur:\n",
    "# IMAGENET_ROOT/train/<klasse>/*.jpg\n",
    "# IMAGENET_ROOT/val/<klasse>/*.jpg\n",
    "\n",
    "imagenet_train = load_dataset(\n",
    "    \"imagefolder\",\n",
    "    data_dir=f\"{IMAGENET_ROOT}/train\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "imagenet_val = load_dataset(\n",
    "    \"imagefolder\",\n",
    "    data_dir=f\"{IMAGENET_ROOT}/val\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "imagenet_train, imagenet_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5189c9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Labels für ImageNet\n",
    "# =============================\n",
    "\n",
    "imagenet_label_names = imagenet_train.features[\"label\"].names\n",
    "num_imagenet_labels = len(imagenet_label_names)\n",
    "\n",
    "id2label_imagenet = {i: name for i, name in enumerate(imagenet_label_names)}\n",
    "label2id_imagenet = {name: i for i, name in enumerate(imagenet_label_names)}\n",
    "\n",
    "num_imagenet_labels, list(id2label_imagenet.items())[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb627799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Processor / Normalisierung\n",
    "# =============================\n",
    "\n",
    "# Processor von einem existierenden ViT holen (nur für Preprocessing!)\n",
    "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "image_size = processor.size.get(\"height\", IMAGE_SIZE)\n",
    "image_mean = processor.image_mean\n",
    "image_std = processor.image_std\n",
    "\n",
    "image_size, image_mean, image_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef981e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Transforms & Preprocessing\n",
    "# =============================\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "# einfache Augmentierung\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=image_mean, std=image_std),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=image_mean, std=image_std),\n",
    "])\n",
    "\n",
    "def transform_imagenet_train(examples):\n",
    "    examples[\"pixel_values\"] = [\n",
    "        train_transform(img.convert(\"RGB\")) for img in examples[\"image\"]\n",
    "    ]\n",
    "    return examples\n",
    "\n",
    "def transform_imagenet_val(examples):\n",
    "    examples[\"pixel_values\"] = [\n",
    "        val_transform(img.convert(\"RGB\")) for img in examples[\"image\"]\n",
    "    ]\n",
    "    return examples\n",
    "\n",
    "# Datasets transformieren\n",
    "imagenet_train = imagenet_train.with_transform(transform_imagenet_train)\n",
    "imagenet_val = imagenet_val.with_transform(transform_imagenet_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd54759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Collate-Funktion\n",
    "# =============================\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in batch])\n",
    "    labels = torch.tensor([example[\"label\"] for example in batch])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a056a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# ViT-Config mit neuer Patch-Size (ImageNet)\n",
    "# =============================\n",
    "\n",
    "# ViTConfig von einem existierenden Modell laden und PATCH_SIZE überschreiben\n",
    "base_config = ViTConfig.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "vit_imagenet_config = ViTConfig(\n",
    "    hidden_size=base_config.hidden_size,\n",
    "    num_hidden_layers=base_config.num_hidden_layers,\n",
    "    num_attention_heads=base_config.num_attention_heads,\n",
    "    intermediate_size=base_config.intermediate_size,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    patch_size=PATCH_SIZE,                 # <--- WICHTIG: neue Patch-Size\n",
    "    num_labels=num_imagenet_labels,\n",
    "    id2label=id2label_imagenet,\n",
    "    label2id=label2id_imagenet,\n",
    ")\n",
    "\n",
    "vit_imagenet_model = ViTForImageClassification(vit_imagenet_config)\n",
    "\n",
    "vit_imagenet_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b2d606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# TrainingArguments & Trainer (ImageNet)\n",
    "# =============================\n",
    "\n",
    "imagenet_training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_IMAGENET_MODEL_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=IMAGENET_NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=100,\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics_imagenet(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return accuracy_metric.compute(predictions=preds, references=labels)\n",
    "\n",
    "imagenet_trainer = Trainer(\n",
    "    model=vit_imagenet_model,\n",
    "    args=imagenet_training_args,\n",
    "    train_dataset=imagenet_train,\n",
    "    eval_dataset=imagenet_val,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics_imagenet,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa91e783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# ImageNet trainieren & speichern\n",
    "# =============================\n",
    "\n",
    "imagenet_trainer.train()\n",
    "\n",
    "# bestes Modell speichern\n",
    "imagenet_trainer.save_model(OUTPUT_IMAGENET_MODEL_DIR)\n",
    "processor.save_pretrained(OUTPUT_IMAGENET_MODEL_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc6078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Phase B: Animal-Dataset laden\n",
    "# =============================\n",
    "\n",
    "animal_dataset = load_dataset(\"imagefolder\", data_dir=ANIMAL_ROOT)\n",
    "\n",
    "animal_train = animal_dataset[\"train\"]\n",
    "# falls ein \"test\" Split existiert, wird er genutzt; sonst None\n",
    "animal_test = animal_dataset.get(\"test\", None)\n",
    "\n",
    "animal_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d625e80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Labels für dein Animal-Dataset\n",
    "# =============================\n",
    "\n",
    "animal_label_names = animal_train.features[\"label\"].names\n",
    "num_animal_labels = len(animal_label_names)\n",
    "\n",
    "id2label_animals = {i: name for i, name in enumerate(animal_label_names)}\n",
    "label2id_animals = {name: i for i, name in enumerate(animal_label_names)}\n",
    "\n",
    "num_animal_labels, id2label_animals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e332c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Transforms für dein Dataset\n",
    "# =============================\n",
    "\n",
    "# Wir benutzen dieselben Augmentierungen / Normalisierung wie oben\n",
    "\n",
    "def transform_animal(examples):\n",
    "    examples[\"pixel_values\"] = [\n",
    "        train_transform(img.convert(\"RGB\")) for img in examples[\"image\"]\n",
    "    ]\n",
    "    return examples\n",
    "\n",
    "animal_train = animal_train.with_transform(transform_animal)\n",
    "\n",
    "if animal_test is not None:\n",
    "    animal_test = animal_test.with_transform(transform_animal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97150af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Modell von ImageNet-Checkpoint laden (from_pretrained)\n",
    "# =============================\n",
    "\n",
    "vit_animals_model = ViTForImageClassification.from_pretrained(\n",
    "    OUTPUT_IMAGENET_MODEL_DIR,\n",
    "    num_labels=num_animal_labels,\n",
    "    id2label=id2label_animals,\n",
    "    label2id=label2id_animals,\n",
    "    ignore_mismatched_sizes=True,  # falls Kopfgröße nicht passt\n",
    ")\n",
    "\n",
    "vit_animals_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32fc288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# TrainingArguments & Trainer (Animals)\n",
    "# =============================\n",
    "\n",
    "animal_training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_ANIMAL_MODEL_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=ANIMAL_NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    evaluation_strategy=\"epoch\" if animal_test is not None else \"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=animal_test is not None,\n",
    "    metric_for_best_model=\"accuracy\" if animal_test is not None else None,\n",
    ")\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics_animals(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return accuracy_metric.compute(predictions=preds, references=labels)\n",
    "\n",
    "animal_trainer = Trainer(\n",
    "    model=vit_animals_model,\n",
    "    args=animal_training_args,\n",
    "    train_dataset=animal_train,\n",
    "    eval_dataset=animal_test if animal_test is not None else None,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics_animals if animal_test is not None else None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac05fb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Fein-Tuning auf dein Dataset & speichern\n",
    "# =============================\n",
    "\n",
    "animal_trainer.train()\n",
    "\n",
    "animal_trainer.save_model(OUTPUT_ANIMAL_MODEL_DIR)\n",
    "processor.save_pretrained(OUTPUT_ANIMAL_MODEL_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132465ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Evaluation & Report\n",
    "# =============================\n",
    "\n",
    "if animal_test is not None:\n",
    "    predictions = animal_trainer.predict(animal_test)\n",
    "\n",
    "    labels_true = predictions.label_ids\n",
    "    labels_pred = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "    report = classification_report(\n",
    "        labels_true,\n",
    "        labels_pred,\n",
    "        target_names=animal_label_names,\n",
    "        output_dict=True,\n",
    "    )\n",
    "\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    print(\"\\n\\nscikit-learn report:\\n\", report_df)\n",
    "\n",
    "    report_df.to_csv(f\"{OUTPUT_ANIMAL_MODEL_DIR}/classification_report.csv\", index=True)\n",
    "\n",
    "    cm = confusion_matrix(labels_true, labels_pred)\n",
    "    cm\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language": "python",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
